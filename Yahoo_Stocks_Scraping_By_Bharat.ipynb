{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Yahoo Stock Data Scraping and Cleaning Process**\n",
        "\n",
        "This code automates the process of scraping stock data from Yahoo Finance, focusing on \"Trending Tickers\" and \"Most Active\" stocks.\n",
        "\n",
        "The steps include:--\n",
        "\n",
        "1. **Accessing the URL:** Opens the browser, waits for the page to load.\n",
        "\n",
        "2. **Navigating the Menu:** Hovering and clicking through the \"Trending Tickers\" and \"Most Active\" sections.\n",
        "\n",
        "3. **Extracting Data:** Scrapes stock information (symbol, name, price, etc.) and navigates through multiple pages.\n",
        "\n",
        "4. **Data Cleaning:** Cleans the scraped data, handles missing values, converts columns to numeric types, and exports it to an Excel file.\n",
        "\n",
        "This process is automated using Selenium for web interaction and Pandas for data cleaning and export."
      ],
      "metadata": {
        "id": "S8mQV-JASCmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from selenium import webdriver                                    # Main Selenium module for browser automation\n",
        "from selenium.webdriver.common.by import By                       # Locates elements on the page\n",
        "from selenium.webdriver.common.keys import Keys                   # Handles keyboard actions (e.g., pressing Enter)\n",
        "from selenium.webdriver.support.select import Select              # Manages drop-down menus in forms\n",
        "from selenium.webdriver.chrome.options import Options             # Configures Chrome options (e.g., headless mode)\n",
        "from selenium.webdriver.support.ui import WebDriverWait           # Waits for elements to be ready before interaction\n",
        "from webdriver_manager.chrome import ChromeDriverManager          # Automatically manages ChromeDriver installation\n",
        "from selenium.webdriver.common.action_chains import ActionChains  # Simulates complex user actions (e.g., mouse movements)\n",
        "from selenium.webdriver.support import expected_conditions as EC  # Waits for specific conditions to occur before continuing\n",
        "\n",
        "driver = webdriver.Chrome()\n",
        "driver.maximize_window()\n",
        "#explicit wait\n",
        "wait = WebDriverWait(driver, 5)\n",
        "\n",
        "#function to load for page\n",
        "def wait_for_page_load(driver , wait):\n",
        "  title = driver.title\n",
        "  try:\n",
        "    wait.until(lambda c : c.execute_script('return document.readyState') == 'complete') # Check if the page has fully loaded\n",
        "  except:\n",
        "    print(f\"Page {title} is not Loaded completely.\\n\")\n",
        "  else:\n",
        "    print(f\"Page {title} loaded succesfully,\\n\")\n",
        "\n",
        "url= \"https://finance.yahoo.com/\"\n",
        "driver.get(url)\n",
        "wait_for_page_load(driver,wait)\n",
        "\n",
        "#Hovering the Market Menu\n",
        "actions = ActionChains(driver)   # Set up actions for the driver\n",
        "market_menu = wait.until(\n",
        "    EC.presence_of_element_located((By.XPATH, '/html/body/div[2]/header/div/div/div/div[4]/div/div/ul/li[3]/a/span')))\n",
        "actions.move_to_element(market_menu).perform()\n",
        "\n",
        "\n",
        "#Check if the trending tickers link is clickable, then click it\n",
        "trending_tickers = wait.until(\n",
        "     EC.element_to_be_clickable((By.XPATH, '/html/body/div[2]/header/div/div/div/div[4]/div/div/ul/li[3]/div/ul/li[4]/a/div')))\n",
        "trending_tickers.click()\n",
        "wait_for_page_load(driver,wait)\n",
        "\n",
        "\n",
        "#Check if the most active link is clickable, then click it\n",
        "most_active = wait.until(\n",
        "    EC.element_to_be_clickable((By.XPATH, '/html/body/div[2]/main/section/section/section/article/section[1]/div/nav/ul/li[1]/a/span')))\n",
        "most_active.click()\n",
        "wait_for_page_load(driver,wait)\n",
        "\n",
        "\n",
        "#Navigating the stockes pages\n",
        "stocks_data = []\n",
        "while True:\n",
        "  #1. scrap the data\n",
        "  # Wait until the table is present\n",
        "  wait.until(\n",
        "      EC.presence_of_element_located((By.TAG_NAME, \"table\")))\n",
        "  rows = driver.find_elements(By.CSS_SELECTOR, \"table tbody tr\") #Find all rows in the table body\n",
        "  for row in rows:   #Loop through each row in the table\n",
        "    values = row.find_elements(By.TAG_NAME, \"td\")\n",
        "    stocks = {\n",
        "              \"symbol\" : values[0].text,     # Stock symbol\n",
        "              \"name\" : values[1].text,       # Stock name\n",
        "              \"price\": values[4].text,       # Stock price\n",
        "              \"change\": values[4].text,      # Stock price change\n",
        "              \"volume\": values[6].text,      # Stock volume\n",
        "              \"market_cap\": values[8].text,  # Market capitalization\n",
        "              \"pe_ratio\" : values[9].text,   # Price-to-earnings ratio\n",
        "    }\n",
        "    stocks_data.append(stocks)\n",
        "  #2. check if next arrow butoon clickable\n",
        "  try:\n",
        "    next_button  = wait.until(\n",
        "        EC.element_to_be_clickable((By.XPATH , '//*[@id=\"nimbus-app\"]/section/section/section/article/section[1]/div/div[3]/div[3]/button[3]')))\n",
        "  except:\n",
        "    print(f\"the next button is not clickable. we have nevigated throgh all the pages\")\n",
        "    break # Exit the loop if no next page is available\n",
        "  else:\n",
        "    next_button.click()\n",
        "    time.sleep(1)\n",
        "driver.quit()\n",
        "\n",
        "# CLEANING THE DATA>>>>>>>>>>>>>>>>\n",
        "df = (pd\n",
        "    .DataFrame(stocks_data)\n",
        "    .apply(lambda col : col.str.strip() if col.dtype == \"object\" else col)     #Remove leading/trailing spaces from string columns\n",
        "    .assign(price =  lambda p : pd.to_numeric(p.price),                        #Convert the 'price' column to numeric\n",
        "            change = lambda c : pd.to_numeric(c.change.str.replace(\"+\",\"\")),   #Clean and convert 'change' column to numeric\n",
        "            volume = lambda v : pd.to_numeric(v.volume.str.replace(\"M\", \"\")),  #Convert 'volume' to numeric, removing 'M' suffix\n",
        "            market_cap = lambda x: x.market_cap.apply\n",
        "                        ( lambda val :\n",
        "                     float(val.replace(\"B\" , \"\")) if \"B\" in val        #Handle 'B' (billion) values\n",
        "                else float(val.replace(\"T\", \"\"))  *1000 if \"T\" in val  #Handle 'T' (trillion) values\n",
        "                else float(val.replace(\"M\", \"\")) * 0.001 if \"M\" in val #Handle 'M' (million) values\n",
        "                else val),\n",
        "            pe_ratio = lambda pe : (pe  # Clean and convert 'pe_ratio'\n",
        "                                      .pe_ratio\n",
        "                                      .replace(\"-\", np.nan)\n",
        "                                      .str.replace(\",\",\"\")\n",
        "                                      .pipe(lambda col : pd.to_numeric(col))))\n",
        "    .rename(columns = {\n",
        "        \"price\" : \"Price_USDT\",\n",
        "        \"volume\" : \"Volume_M\",\n",
        "        \"market_cap\" : \"Market_Cap_B\"})\n",
        "   )\n",
        "df\n",
        "#df.price.str.extract(r\"([0-9.])\", expand = False).unique()  >> Uncomment to check unique values in the 'price' column apart from numbers and \".\"\n",
        "df.to_excel(\"Yahoo stocks scraped data by bharat.xlsx\", index = False)"
      ],
      "metadata": {
        "id": "Oy6Zs2wT1Nw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Object-oriented programming** (OOP) is important for organizing code, making it more modular, reusable, and maintainable. By encapsulating each step—like **accessing** the URL, **navigating** stocks, **extracting** data, and **cleaning** it—into separate methods or classes, **OOP improves structure, reduces redundancy**, and simplifies future modifications or debugging, even if the initial code isn't OOP-based.\n",
        "\n",
        "\n",
        "**Divided Completed Process in 4 Steps Which Will Be Methods of Scraper Class:-**\n",
        "\n",
        "\n",
        "**| Accessing the Main URL |**\n",
        "\n",
        "1. Open the browser and maximize the window.\n",
        "\n",
        "2. Initialize the Explicit Wait instance.\n",
        "\n",
        "3. Access the main URL and wait for the page to load.\n",
        "\n",
        "\n",
        "**| Accessing Most Active Stocks |**\n",
        "\n",
        "4. Hover over the \"Markets Menu.\"\n",
        "\n",
        "5. Click on \"Trending Tickers.\"\n",
        "\n",
        "6. Click on \"Most Active.\"\n",
        "\n",
        "\n",
        "**| Extracting the Stocks Data |**\n",
        "\n",
        "7. Scrape the data as dictionaries.\n",
        "\n",
        "8. Navigate until the last page for more data.\n",
        "\n",
        "9. Store the extracted data in a list.\n",
        "\n",
        "10. Close the browser.\n",
        "\n",
        "**| Clean & Save the Data |**\n",
        "\n",
        "11. Clean the extracted data.\n",
        "\n",
        "12. Export the cleaned data as an Excel file.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7FiKcDik2TdE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7_OXa0m8WmnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from selenium import webdriver                                    # Main Selenium module for browser automation\n",
        "from selenium.webdriver.common.by import By                       # Locates elements on the page\n",
        "from selenium.webdriver.common.keys import Keys                   # Handles keyboard actions (e.g., pressing Enter)\n",
        "from selenium.webdriver.support.select import Select              # Manages drop-down menus in forms\n",
        "from selenium.webdriver.chrome.options import Options             # Configures Chrome options (e.g., headless mode)\n",
        "from selenium.webdriver.support.ui import WebDriverWait           # Waits for elements to be ready before interaction\n",
        "from webdriver_manager.chrome import ChromeDriverManager          # Automatically manages ChromeDriver installation\n",
        "from selenium.webdriver.common.action_chains import ActionChains  # Simulates complex user actions (e.g., mouse movements)\n",
        "from selenium.webdriver.support import expected_conditions as EC  # Waits for specific conditions to occur before continuing\n",
        "\n",
        "class Scraping:\n",
        "  def __init__(self, driver, timeout = 10):\n",
        "    self.driver = driver\n",
        "    self.wait = WebDriverWait(self.driver, timeout = timeout)\n",
        "    self.stocks_data = []\n",
        "\n",
        "  #Method-1------------------------------------------------>\n",
        "  def wait_for_page_load(self):\n",
        "    page_of_title = self.driver.title\n",
        "    try:\n",
        "      self.wait.until(lambda c : c.execute_script('return document.readyState') == 'complete')\n",
        "    except:\n",
        "      print(f\"Page {page_of_title} is not Loaded completely.\\n\")\n",
        "    else:\n",
        "      print(f\"Page {page_of_title} loaded succesfully.\\n\")\n",
        "\n",
        "  def Access_url(self, url):\n",
        "    self.driver.get(url)\n",
        "    self.wait_for_page_load()\n",
        "\n",
        "  #Method 2------------------------------------------------>\n",
        "  def Accessing_most_active_stocks(self):\n",
        "\n",
        "    #Hovering the Market Menu by using XPATH of Market Menu\n",
        "    actions = ActionChains(self.driver)\n",
        "    market_menu = self.wait.until(\n",
        "      EC.presence_of_element_located((By.XPATH, '/html/body/div[2]/header/div/div/div/div[4]/div/div/ul/li[3]/a/span')))\n",
        "    actions.move_to_element(market_menu).perform()\n",
        "\n",
        "    #Click on trending clickers by using XPATH of Trending clickers\n",
        "    trending_tickers = self.wait.until(\n",
        "        EC.element_to_be_clickable((By.XPATH, '/html/body/div[2]/header/div/div/div/div[4]/div/div/ul/li[3]/div/ul/li[4]/a/div')))\n",
        "    trending_tickers.click()\n",
        "    self.wait_for_page_load()\n",
        "\n",
        "    #Click on most active by using XPATH of most active\n",
        "    most_active = self.wait.until(\n",
        "      EC.element_to_be_clickable((By.XPATH, '/html/body/div[2]/main/section/section/section/article/section[1]/div/nav/ul/li[1]/a/span')))\n",
        "    most_active.click()\n",
        "    self.wait_for_page_load()\n",
        "\n",
        "  #Method 3-------------------------------------------------->\n",
        "  def Extracting_data(self):\n",
        "    while True:\n",
        "\n",
        "      #scrap the data from webpage\n",
        "      self.wait.until(\n",
        "          EC.presence_of_element_located((By.TAG_NAME, \"table\")) # table tag name\n",
        "                      )\n",
        "      rows = self.driver.find_elements(By.CSS_SELECTOR, \"table tbody tr\") # these are tag hieararchy\n",
        "      for row in rows:\n",
        "        values = row.find_elements(By.TAG_NAME, \"td\") # tag name of row\n",
        "        stocks = {\n",
        "                  \"symbol\" : values[0].text,\n",
        "                  \"name\" : values[1].text,\n",
        "                  \"price\": values[3].text,\n",
        "                  \"change\": values[4].text,\n",
        "                  \"volume\": values[6].text,\n",
        "                  \"market_cap\": values[8].text,\n",
        "                  \"PE_Ratio\" : values[9].text,\n",
        "                 }\n",
        "        self.stocks_data.append(stocks)\n",
        "\n",
        "      #check if next arrow butoon clickable by providing \">\" this button XPATH\n",
        "      try:\n",
        "        next_button  = self.wait.until(\n",
        "            EC.element_to_be_clickable((By.XPATH , '//*[@id=\"nimbus-app\"]/section/section/section/article/section[1]/div/div[3]/div[3]/button[3]'))\n",
        "                                  )\n",
        "      except:\n",
        "        print(f\"the next button is not clickable. we have nevigated throgh all pages\")\n",
        "        break\n",
        "      else:\n",
        "        next_button.click()\n",
        "        time.sleep(1)\n",
        "\n",
        "  #Method 4-------------------------------------------------------->\n",
        "  def Data_Cleaning(self, filename=\"temp\"):\n",
        "      df = (pd\n",
        "        .DataFrame(stocks_data)\n",
        "        .apply(lambda col : col.str.strip() if col.dtype == \"object\" else col)     #Remove leading/trailing spaces from string columns\n",
        "        .assign(price =  lambda p : pd.to_numeric(p.price),                        #Convert the 'price' column to numeric\n",
        "                change = lambda c : pd.to_numeric(c.change.str.replace(\"+\",\"\")),   #Clean and convert 'change' column to numeric\n",
        "                volume = lambda v : pd.to_numeric(v.volume.str.replace(\"M\", \"\")),  #Convert 'volume' to numeric, removing 'M' suffix\n",
        "                market_cap = lambda x: x.market_cap.apply\n",
        "                            ( lambda val :\n",
        "                         float(val.replace(\"B\" , \"\")) if \"B\" in val        #Handle 'B' (billion) values\n",
        "                    else float(val.replace(\"T\", \"\"))  *1000 if \"T\" in val  #Handle 'T' (trillion) values\n",
        "                    else float(val.replace(\"M\", \"\")) * 0.001 if \"M\" in val #Handle 'M' (million) values\n",
        "                    else val),\n",
        "                pe_ratio = lambda pe : (pe  # Clean and convert 'pe_ratio'\n",
        "                                          .pe_ratio\n",
        "                                          .replace(\"-\", np.nan)\n",
        "                                          .str.replace(\",\",\"\")\n",
        "                                          .pipe(lambda col : pd.to_numeric(col))))\n",
        "        .rename(columns = {\n",
        "            \"price\" : \"Price_USDT\",\n",
        "            \"volume\" : \"Volume_M\",\n",
        "            \"market_cap\" : \"Market_Cap_B\"})\n",
        "           )\n",
        "#df.price.str.extract(r\"([0-9.])\", expand = False).unique()  >> Uncomment to check unique values in the 'price' column apart from numbers and \".\"\n",
        "      df.to_excel(f\"{filename}.xlsx\", index = False)\n",
        "\n",
        "if __name__ == \"__main__\":      # we are implemeting code of ourself module\n",
        "  driver = webdriver.Chrome()\n",
        "  driver.maximize_window()\n",
        "  url = 'https://finance.yahoo.com/'\n",
        "  scraper = Scraping(driver , 5)\n",
        "  scraper.Access_url(url)\n",
        "  scraper.Accessing_most_active_stocks()\n",
        "  scraper.Extracting_data()\n",
        "  scraper.Data_Cleaning(\"Yahoo_stocks_scraped_data_by_Bharat\")\n",
        "  driver.quit()\n",
        "#END---------------------xx--------------------xx-------------------------xx-------------------------xx"
      ],
      "metadata": {
        "id": "2z5IZ4J12Qrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W7ax-aboWE-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8-3x8IKvWF_O"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}